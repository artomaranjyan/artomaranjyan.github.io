@article{maranjyan2025ringmaster,
  selected={true},
  title={{R}ingmaster {ASGD}: The First {A}synchronous {SGD} with Optimal Time Complexity}, 
  author={Artavazd Maranjyan and Alexander Tyurin and Peter Richtárik},
  journal={arXiv:2501.16168},
  year={2025},
  html={https://arxiv.org/abs/2501.16168},
  url={https://arxiv.org/abs/2501.16168},
  slides={Ringmaster_FLOW.pdf},
  poster={posters/Ringmaster_ICML.pdf},
  video={https://youtu.be/DJI1SWfG6ME?si=VJGtD1G840gO5d-w},
  abstract={Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richtárik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.}, 
}

@article{maranjyan2025ata,
  selected={true},
  title={{ATA}: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning}, 
  author={Artavazd Maranjyan and El Mehdi Saad and Peter Richtárik and Francesco Orabona},
  journal={arXiv:2502.00775},
  year={2025},
  html={https://arxiv.org/abs/2502.00775}, 
  url={https://arxiv.org/abs/2502.00775}, 
  poster={posters/ATA_ICML.pdf},
  abstract={Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers.}
}

@inproceedings{maranjyan2025mindflayer,
  selected={true},
  title={MindFlayer {SGD}: Efficient Parallel {SGD} in the Presence of Heterogeneous and Random Worker Compute Times},
  author={Artavazd Maranjyan and Omar Shaikh Omar and Peter Richt{\'a}rik},
  booktitle={The 41st Conference on Uncertainty in Artificial Intelligence},
  year={2025},
  html={https://openreview.net/forum?id=RNpvu3MSvm},
  url={https://openreview.net/forum?id=RNpvu3MSvm},
  poster={posters/MindFlayer_UAI.pdf},
  slides={MindFlayer_AppleMLR.pdf},
  video={https://neurips.cc/virtual/2024/100410},
  abstract={We investigate the problem of minimizing the expectation of smooth nonconvex functions in a distributed setting with multiple parallel workers that are able to compute stochastic gradients. A significant challenge in this context is the presence of arbitrarily heterogeneous and stochastic compute times among workers, which can severely degrade the performance of existing parallel stochastic gradient descent (SGD) methods. While some parallel SGD algorithms achieve optimal performance under deterministic but heterogeneous delays, their effectiveness diminishes when compute times are random—a scenario not explicitly addressed in their design. To bridge this gap, we introduce MindFlayer SGD, a novel parallel SGD method specifically designed to handle stochastic and heterogeneous compute times. Through theoretical analysis and empirical evaluation, we demonstrate that MindFlayer SGD consistently outperforms existing baselines, particularly in environments with heavy-tailed noise. Our results highlight its robustness and scalability, making it a compelling choice for large-scale distributed learning tasks.}
}

@article{maranjyan2025gradskip,
  selected={true},
  title={{GradSkip}: Communication-Accelerated Local Gradient Methods with Better Computational Complexity},
  author={Artavazd Maranjyan and Mher Safaryan and Peter Richt{\'a}rik},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2025},
  url={https://openreview.net/forum?id=6R3fRqFfhn},
  html={https://openreview.net/forum?id=6R3fRqFfhn},
  poster={GradSkip_Rising_Stars.pdf},
  slides={GradSkip_Warwick.pdf},
  video={https://www.youtube.com/watch?v=WWhY5tO-FiM},
  abstract={We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing clients to perform multiple local gradient-type training steps before communication. In a recent breakthrough, Mishchenko et al. (2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their ProxSkip method requires all clients to take the same number of local training steps in each communication round. We propose a redesign of the ProxSkip method, allowing clients with ``less important'' data to get away with fewer local training steps without impacting the overall communication complexity of the method. In particular, we prove that our modified method, GradSkip, converges linearly under the same assumptions and has the same accelerated communication complexity, while the number of local gradient steps can be reduced relative to a local condition number. We further generalize our method by extending the randomness of probabilistic alternations to arbitrary unbiased compression operators and by considering a generic proximable regularizer. This generalization, which we call GradSkip+, recovers several related methods in the literature as special cases. Finally, we present an empirical study on carefully designed toy problems that confirm our theoretical claims.}
}
@inproceedings{condat2024locodl,
  title={{LoCoDL}: Communication-Efficient Distributed Learning with Local Training and Compression},
  author={Condat, Laurent and Maranjyan, Artavazd and Richt{\'a}rik, Peter},
  booktitle={ICLR 2025: The Thirteenth International Conference on Learning Representations},
  year={2025},
  html={https://arxiv.org/abs/2403.04348},
  url={https://arxiv.org/abs/2403.04348},
  poster={Locodl_poster.pdf},
  video={https://iclr.cc/virtual/2025/poster/29728},
  abstract={In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.}
}
@article{maranjyan2024differentially,
  title={Differentially Private Random Block Coordinate Descent},
  author={Maranjyan, Artavazd and Sadiev, Abdurakhmon and Richt{\'a}rik, Peter},
  journal={OPT 2024: Optimization for Machine Learning (NeurIPS workshop)},
  year={2024},
  html={https://arxiv.org/abs/2412.17054},
  url={https://arxiv.org/abs/2412.17054},
  poster={DP-CD_Neurips2024.pdf},
  abstract={Coordinate Descent (CD) methods have gained significant attention in machine learning due to their effectiveness in solving high-dimensional problems and their ability to decompose complex optimization tasks. However, classical CD methods were neither designed nor analyzed with data privacy in mind, a critical concern when handling sensitive information. This has led to the development of differentially private CD methods, such as DP-CD (Differentially Private Coordinate Descent) proposed by Mangold et al. (ICML 2022), yet a disparity remains between non-private CD and DP-CD methods. In our work, we propose a differentially private random block coordinate descent method that selects multiple coordinates with varying probabilities in each iteration using sketch matrices. Our algorithm generalizes both DP-CD and the classical DP-SGD (Differentially Private Stochastic Gradient Descent), while preserving the same utility guarantees. Furthermore, we demonstrate that better utility can be achieved through importance sampling, as our method takes advantage of the heterogeneity in coordinate-wise smoothness constants, leading to improved convergence rates.}
}

@article{grigoryan2023menshov,
  title={{Menshov}-Type Theorem for Divergence Sets of Sequences of Localized Operators},
  journal={Journal of Contemporary Mathematical Analysis (Armenian Academy of Sciences)},
  author={Grigoryan, Martin and Kamont, Anna and Maranjyan, Artavazd},
  volume={58},
  number={2},
  pages={81--92},
  year={2023},
  publisher={Springer}
}
@article{grigoryan2021divergence,
  title={On the divergence of {F}ourier series in the general {Haar} system},
  author={Grigoryan, Martin and Maranjyan, Artavazd},
  journal={Armenian Journal of Mathematics},
  volume={13},
  number={6},
  pages={1--10},
  year={2021}
}
@article{grigoryan2021unconditional,
  title={ON THE UNCONDITIONAL CONVERGENCE OF {F}ABER-{S}CHAUDER SERIES IN $ L\^{}$\{$1$\}$ $},
  author={Grigoryan, Tigran M and Maranjyan, Artavazd},
  journal={Proceedings of the YSU A: Physical and Mathematical Sciences},
  volume={55},
  number={1 (254)},
  pages={12--19},
  year={2021}
}

